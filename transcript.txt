With Serrano and this is Serrano Academy and in this video I will tell you about KL divergence and cross entropy. In statistics, machine learning and similar fields, you're going to be working with a lot of probability distributions. And many times you're going to want to know how different two distributions are from each other. A very useful and popular way of doing this is using the callback, libeler divergence, or KL divergence for short. Libeler divergence has this formula, which is related to the formula for cross entropy.  entropy. However, I'm not a big fan of sums of logarithms. I like to see everything as a probability. So in this video, I'm going to show you a game and calculating the probability of this game is going to get us really close to calculating the cross entropy and the KL divergence. Are you ready? Let's begin. So let's start by considering a five-sided die with sides labeled A, B, C, D, and E. And I know it's hard to make a five-sided die in real life, but that's just pretend we have one. Now I'm going to toss this die.  10 times and I get the following. First I get that it lands on A, then A again A, A, then B, B, C, D, E, and then E. So those were my results and here's the histogram of results. Four times A, two times B, one times C, one times D, and two times E. Now this doesn't look like a fair die, it looks like a biased one. And from here we can guess that maybe the probabilities are 40...  for A, 20% for B, 10% for C, and for D, and then 20% for E. And now let's say that these are the actual probabilities, that we toss the die thousands of times and we realize that it's biased in this way. These are the probabilities for it to land in any one of the faces. And of course, we can continue tossing it and we get different results. But if we toss it many times, we're gonna get 40% at length and A, 20% at B, and on E, and then 10% on C, and on D. Now the question is the following. Let's say that...  I want to toss this die and replicate the sequence. So I want to get a a a a b b c d and e that is not easy, even though the die has those probabilities is still hard to get down. But let's say we win a lot of money if we're able to replicate the sequence. Now I'm going to ask you a question. If we want to replicate the sequence, which of the following dies is a better choice. So the first one is the original die, the one that has these probabilities. Then we have another die that is kind of similar, but it has these other probabilities. And then finally we have a third die, which has  these probabilities for ABCD and E. Now which one would you pick? Well, if you set the first one, you are right. And which one is the second one that you would pick? And if you set the second one, you are right. And it seems like the third die is the worst one. And why is this the case? Well, your intuition may have told you that if the distribution for the results that we want to get, it's on the top left, is 0.4.2.1.1.1.2. Well,  which one of the bottom ones is the most similar to that one? Well, the first one is exactly the same, so that should give us the best probability for getting that sequence on top. The second one is pretty similar, so it should give us a decent probability, and the third one is very dissimilar. In fact, it's going to land on C a lot more, so it's very unlikely it's going to give us the original sequence it could, but it's very likely that it won't. But it seems like we're thinking a lot about probabilities. What is the probability?  of getting this sequence. That's what we're going to calculate today. And these probabilities are very small. However, the first one is going to give us the highest possible probability. The second one is going to give us a slightly lower probability. And the third one is going to give us a much, much lower probability of obtaining the sequence above. So let's start by calculating the probability of obtaining this sequence on top using die one. Remember that die one had these probabilities for landing in a, b, c, t, and e. So first let's tie the probability of each independent toss. So forward.  The first four is the probability of each one of them is 0.4. Then for the next two Bs is 0.24 each. For the C is 0.1. For the D is 0.1. And for the last two E's is 0.2 each. Now notice that each toss is completely independent from the other ones because we picked up the die and tossed it again. And so therefore the probability that all the tosses land on these letters is the product of all of them. Because when we have independent events, the probability that all of them happen is the product of all the independent probabilities. This is a time.  number is 0.000004096. However, it's the best we can do. If I pick any other tie, the probability is going to be even smaller. Now the procedure I'm going to do is related to something called entropy. If you want to know more about it, there's a video here on my channel called Shannon Entropy and Information Game. But basically the main thing of entropy is the following. I don't like numbers that are so small. And I don't like products of a lot of numbers.  In particular, imagine if you have the product of thousands of tiny numbers. This is first of all tiny and second of all very volatile because if I change one number, the product can change a lot. Instead of products, I like sums. And how do you turn products into sums? Well, with a very nice function, the logarithm. So in machine learning, most of the time that you see a logarithm is because somebody was dealing with a huge product and wanted to turn it into a sum. So the logarithm here is minus 14.71.  Now I'm taking the natural logarithm, the logarithm based E, many times the logarithm base 2 is taken or the logarithm base 10. At the end of the day, it doesn't make a difference because if you change logarithm base E by logarithm base 2 or by base 10, all you have to do is multiply everything by a constant. So when you do logarithms, as long as you're consistent and always use the right base, most of the time, it doesn't really matter what base we use. But if you do this and start getting different numbers, it...  maybe because you're taking a different base. Now the logarithm turns products into sums. So now we have instead of logarithm of this product, we have logarithm of point four plus logarithm of point four plus all the way to logarithm of point two. It's a sum of ten things. So we can write it as four logarithm of point four plus two logarithm of point two plus one logarithm of point one plus one logarithm of point one plus two logarithm of point two. And let's take the average of those. So let's divide it by ten because there's ten numbers. And because we divide by ten, then we have minus one point.  4, 7, 1. Notice that these logarithms are always going to be negative. Why are they negative? Because it's the logarithm of something that's between 0 and 1. And the logarithm of numbers between 0 and 1 is always going to be negative. The logarithm of 1 is 0. And then the logarithm for numbers that are bigger than 1 is positive. So we're always going to end up dealing with negative numbers. But we can clean this up a bit by multiplying both sides by negative 1. And so we have that 1.471 is the negative of this average. Let's write it again over here. We have negative 1.471 is the negative of this average. Let's write it again.  0.4 log of 0.4 minus 0.2 log of 0.2 minus 0.1 log of 0.1 minus 0.1 log of 0.1 minus 0.2 log of 0.2. Why did I write like this? Because here we have two distributions. It looks like one of them, but in reality there's two. There's a distribution that we're going to call p, which is the one coming from the sequence, and there's a distribution that we're also going to call p, which is the one coming from the die. In this case, they're the same, but they're not always going to be the same. And when we add them all and we get the negative sum of pi.  log of pi, we get 1.471. This is called the cross entropy and it's denoted as h of p and p, where p is the first distribution, the one corresponding to the sequence we want to get, and then the second p is the second distribution, the one corresponding to the die that we're tossing. Now in here, because the two distributions are the same, it's actually called the entropy, and it's denoted as h of p. So when you have a distribution, the entropy is this number. And in general, the more spread out the distribution is the higher the entropy. If you have  distribution where you only have a few values that your variable can hit, then the entropy is small. But if you have many variables and it looks more homogeneous, then the entropy is higher. Now let's do the exact same thing for die number two. This one has these probabilities over here. So they're almost the same as in the first one, but they change a little bit. And we do the same game. So the first four have probably a T.4, then point one, point two, point two, and point one. So the numbers change a little bit to find the probability of all these...  tosses so the probability of obtaining the sequence with die number two we get the product and that's actually a small number is 0.0001024 so it's smaller than the previous one and when we do the logarithms and take the average now we're going to get the negative of that is 1.609 so it's a bigger number because we negated the number so we actually get a smaller number when we take the logarithm but we take the negative so we get a bigger number and when we write it like this then notice that we  still have our two distributions. This is the distribution P that comes from the sequence, which is the exact same one as before, is 0.4, 0.2, 0.1, 0.1, 0.2. And the second distribution is the one coming from die number two. So that's 0.4, 0.1, 0.2, 0.1. So this time they're different. And now our summation is negative, summation of P i logarithm of q i, which is 0.1, 0.609. So now it's not the entropy, now it's called cross entropy, and we are applying it to P and Q. Now let's do the exact same thing for die.  This one should give us as much smaller probability because the distribution here is very different from the one of the sequence But let's see so we have point one for the four a's point two for the b's point four for the c point two for the d And then point one for the e the probability here is super tiny check it out There's a lot of zeros and then a three and a two when we take the logarithm and then the average and then negated We get 1.956 and that comes from here on the left We have p which comes from the sequence  and on the right we have R that comes from the die. And therefore our cross entropy is going to be this formula over here, which is going to be 1.956. So let's do a small summary. Here we have our sequence and the distribution for the values in our sequence and then we have die 1, 2 and 3. For each of the die we calculate the probability that if we throw 10 times we will get the sequence. These numbers are very small, here they are. And as you can see the biggest one is for die 1, which has the...  exact same distribution as the sequence. The one that's slightly similar is the next one and the one that's vastly different is the last one. And actually, di1 will always win. The highest probability we can obtain of obtaining that sequence is using a di that has the exact same distribution as the sequence. Then we get the cross entropy, which is the logarithm of this probability times a negative 1 and scaled. So for the first one, since the distribution is the same, it's called the entropy.  is 1.471. For the second case, we get 1.609 and for the third case, we get 1.956. Actually, the smallest we can get is the entropy. And if we have any other distribution, the cross entropy is larger than the entropy, as you can see. And now to get the KL divergence, we just subtract the cross entropy minus the entropy. So here we get zero because it's the same thing. On here, we get 0.138, which is slightly bigger than zero.  and here we get 0.485 which is a lot bigger than 0. If we want to look at which one has the least entropy it's this one over here. As you can see the smallest kL divergence is between the distribution and itself and as the distribution gets more and more different then the KL divergence grows and grows and it's always positive. So the smallest kL divergence you can ever obtain is 0 which is when you put the same distribution in both sides and as you change the distribution the KL divergence will grow so it's a measure of...  similar to distributions are. Notice one thing that's important that it says not a distance so it's not symmetric because d of pq is not the same as d of qp. But it's still a good measure to tell how different to distributions are. And a summary formula. So here's the most summary of formulas. We have the distribution for the sequence which is called p and the distribution for the probabilities of the die which we're going to call q and the probability of tossing the die and obtaining the sequence is going to be the product of qi to the pi actually times n.  And when we take the cross entropy, then we're taking the logarithm of this probability times minus one over n. The minus is to make it positive and the one over n is to scale it. And the formula is this over here. It's the negative sum of pi log of qi. And finally, the KL divergence is just the change in cross entropy. So it's the cross entropy of pq minus the entropy of p. Now we did this in the discrete case, but we can also do it in the continuous case. So if we have two continuous distributions, p and q, and we want to see how different this is.  are we can still calculate the cross entropy and the KL divergence. So the cross entropy is very similar to the one with discrete distributions except instead of a summation, we have an integral. Here I took the integral over the reals but you can take the integral over any domain where these distributions leave. And the KL divergence is the same thing, is the difference between the cross entropy and the entropy. So that's all folks, thank you very much for your attention. If you liked this video, please subscribe, hit like and throw in a comment. I really like reading your comments.  You can also tweet at me at Serrano Academy or check out my page, Serrano.academy, where there's a lot of blog posts and videos and code and a lot of stuff. And if you want to go deeper, you can take a look at my book, Rocking Machine Learning, in the link in the comments, there is the link and a discount code for 40%. So thank you very much for your attention and see you in the next video.